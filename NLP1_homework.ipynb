{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitriySechkin/ds-learning-sb/blob/main/NLP1_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ML1_1:\n",
        "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
        "\n",
        "###ML1_2:\n",
        "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
        "\n",
        "###ML1_3:\n",
        "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n",
        "\n",
        "###ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
      ],
      "metadata": {
        "id": "dH7qx_irU4Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_1\n",
        "\n",
        "import re\n",
        "\n",
        "patt = r'(ok){3,}'\n",
        "string = 'okokokok'\n",
        "\n",
        "re.match(patt, string)\n"
      ],
      "metadata": {
        "id": "e55YBS3Qlf0U",
        "outputId": "a535531a-2389-4d3b-b1b1-e57046245c82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 8), match='okokokok'>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_2\n",
        "\n",
        "import re\n",
        "\n",
        "pos_test = [\n",
        "  '12-34-56-78',\n",
        "  '12:34:56:78',\n",
        "  '12---34---56---78',\n",
        "  '12.34.56.78'\n",
        " ]\n",
        "\n",
        "neg_test = [\n",
        "  '1-234-56-78',\n",
        "  '12-45.78:10'\n",
        "]\n",
        "\n",
        "\n",
        "patt = r'^(((\\d{2}-){3})|(\\d{2}:){3}|(\\d{2}---){3}|(\\d{2}\\.){3})\\d{2}$'\n",
        "\n",
        "for t in pos_test:\n",
        "  m = re.match(patt, t)\n",
        "  print(m)\n",
        "\n",
        "for n in neg_test:\n",
        "  m = re.match(patt, n)\n",
        "  print(m)"
      ],
      "metadata": {
        "id": "xJfkstKpqsXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed5ba55-3118-4397-e72d-73faaebb48e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 11), match='12-34-56-78'>\n",
            "<re.Match object; span=(0, 11), match='12:34:56:78'>\n",
            "<re.Match object; span=(0, 17), match='12---34---56---78'>\n",
            "<re.Match object; span=(0, 11), match='12.34.56.78'>\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_3:\n",
        "\n",
        "import re\n",
        "\n",
        "n = int(input())\n",
        "\n",
        "patt = r'<a\\shref=\\\"(.*?)\\\".*?>([^<>]*)</'\n",
        "\n",
        "for i in range(n):\n",
        "    string = input()\n",
        "    res = re.findall(patt, string)\n",
        "    if res:\n",
        "        for r in res:\n",
        "            print(r[0].strip(), r[1].strip(), sep=',')"
      ],
      "metadata": {
        "id": "JofiarGKo5uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML1_4: Реализовать stemming, lemmatization & BoW на следующем датасете:\n",
        "!wget https://cld-unzipper3.datacloudmail.ru/unzip/attach/814Mgbwy3s6x14tA43z2pA4z2BC8BEtpBe7dnbasHp83gFqzADkZjgpLSvx4i2CifufdQR/labeled.csv\n"
      ],
      "metadata": {
        "id": "TpnOldhHyo3C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "qZN3NkSyKz0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import itertools\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "import pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XH4MZZL5T4B",
        "outputId": "6f8ebe4d-98b6-4384-8676-25579d78f358"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "with open('labeled.csv', newline='\\n') as csvfile:\n",
        "    csvreader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in csvreader:\n",
        "      data.append(row[0])"
      ],
      "metadata": {
        "id": "kOpr-nWF20E2"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\", \"+\", '\"?']\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "stemmer = SnowballStemmer('russian')\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "data_tok = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "  data_tok.append(tokenizer.tokenize(data[i].lower()))\n"
      ],
      "metadata": {
        "id": "A8T6FaPO293u"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_target_token(token):\n",
        "  return token.isalpha() and \\\n",
        "          token not in special_char and \\\n",
        "            token not in stopwords.words('russian')"
      ],
      "metadata": {
        "id": "lJStBbh8Edjg"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = []\n",
        "\n",
        "for sent in data_tok:\n",
        "  n_sent = [stemmer.stem(morph.parse(t)[0].normal_form) for t in sent if is_target_token(t)]\n",
        "  preprocessed_data.append(n_sent)\n",
        "\n",
        "assert(len(preprocessed_data) == len(data_tok))"
      ],
      "metadata": {
        "id": "Jbgeh0ZwD924"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_vocab = list(set(list(itertools.chain.from_iterable(preprocessed_data))))\n",
        "unique_vocab.remove('')"
      ],
      "metadata": {
        "id": "Wr2OsOvTHa3B"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens, vocab):\n",
        "    vector=[]\n",
        "    for w in vocab:\n",
        "        vector.append(tokens.count(w))\n",
        "    return vector\n",
        "\n",
        "bow = []\n",
        "\n",
        "for i in preprocessed_data:\n",
        "  bow.append(vectorize(i, unique_vocab))\n"
      ],
      "metadata": {
        "id": "FDrFfOMTAsBh"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert(len(data) == len(bow))"
      ],
      "metadata": {
        "id": "2VUOufuP5LOY"
      },
      "execution_count": 141,
      "outputs": []
    }
  ]
}